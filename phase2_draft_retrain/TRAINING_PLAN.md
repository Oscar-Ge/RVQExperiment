# Draft Model é‡æ–°è®­ç»ƒæ–¹æ¡ˆ

## ğŸ¯ ç›®æ ‡

è®­ç»ƒä¸€ä¸ª**åŒ…å«Projection Layerçš„Draft Model**ï¼Œèƒ½å¤Ÿï¼š
1. æ¥å—OpenVLAçš„4096ç»´hidden statesä½œä¸ºè¾“å…¥
2. é¢„æµ‹RFSQçš„å‰3ä¸ªcoarse layersï¼ˆL0, L1, L2ï¼‰
3. è¾¾åˆ°>85%çš„coarse layer accuracy
4. åœ¨Phase 3ä¸­å®ç°1.3-1.6xæ¨ç†åŠ é€Ÿ

---

## ğŸ—ï¸ æ–°æ¶æ„è®¾è®¡

### æ¨¡å‹ç»“æ„

```python
class RFSQDraftModelWithProjection(nn.Module):
    def __init__(
        self,
        input_dim=4096,         # OpenVLA hidden size
        hidden_dim=512,         # Draft model hidden size
        num_coarse_layers=3,    # é¢„æµ‹L0, L1, L2
        chunk_len=8,
        action_hidden_dim=16,   # RFSQ hidden dim
        grid_size=7,            # RFSQ vocab size
    ):
        super().__init__()

        # ğŸ”‘ æ–°å¢ï¼šProjection Layer (4096 â†’ 512)
        self.input_projection = nn.Linear(input_dim, hidden_dim)

        # Transformer Decoderï¼ˆä¿æŒä¸å˜ï¼‰
        self.decoder = DraftTransformerDecoder(
            hidden_dim=hidden_dim,
            num_heads=8,
            feedforward_dim=2048,
        )

        # Classification Headsï¼ˆä¿æŒä¸å˜ï¼‰
        # é¢„æµ‹æ¯ä¸ªcoarse layerçš„tokens
        self.classification_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.GELU(),
                nn.LayerNorm(hidden_dim // 2),
                nn.Linear(hidden_dim // 2, chunk_len * action_hidden_dim * grid_size),
            )
            for _ in range(num_coarse_layers)
        ])

    def forward(self, openvla_hidden_states):
        """
        Args:
            openvla_hidden_states: [Batch, 4096] from OpenVLA

        Returns:
            logits: [Batch, Num_Coarse_Layers=3, Chunk*Hidden=128, Grid=7]
        """
        # Step 1: Project 4096 â†’ 512
        projected = self.input_projection(openvla_hidden_states)  # [B, 512]

        # Step 2: Add sequence dimension
        x = projected.unsqueeze(1)  # [B, 1, 512]

        # Step 3: Transformer Decoder
        decoder_output = self.decoder(x)  # [B, 1, 512]
        decoder_output = decoder_output.squeeze(1)  # [B, 512]

        # Step 4: Predict coarse layers
        layer_outputs = []
        for head in self.classification_heads:
            logits = head(decoder_output)  # [B, 128*7=896]
            # Reshape to [B, Chunk*Hidden=128, Grid=7]
            logits = logits.view(-1, 128, 7)
            layer_outputs.append(logits)

        # Stack: [B, 3, 128, 7]
        return torch.stack(layer_outputs, dim=1)
```

### æ•°æ®æµ

```
LIBERO Episode
  â†“
Observations (image + task description)
  â†“
[OpenVLA Frozen Forward]
  â†“
Hidden States [Batch, Seq, 4096]
  â†“ (å–æœ€åä¸€ä¸ªtoken)
Last Hidden State [Batch, 4096]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Draft Model with Projection         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Projection: [B, 4096] â†’ [B, 512] â”‚
â”‚ 2. Decoder: [B, 1, 512] â†’ [B, 512]  â”‚
â”‚ 3. Heads: [B, 512] â†’ [B, 3, 128, 7] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Predicted Logits [B, 3, 128, 7]
  â†“
Cross-Entropy Loss with Ground Truth
  (Ground Truth from RFSQ encoding)
```

---

## ğŸ“Š è®­ç»ƒé…ç½®

### æ•°æ®é›†

**æ¥æº**ï¼šLIBEROæ•°æ®é›†ï¼ˆlibero_spatialï¼‰

**å¤„ç†æµç¨‹**ï¼š
1. æ”¶é›†episodesï¼ˆä½¿ç”¨OpenVLA rolloutæˆ–å·²æœ‰æ•°æ®ï¼‰
2. å¯¹æ¯ä¸ªobservationï¼š
   - é€šè¿‡**frozen OpenVLA**æå–4096ç»´hidden states
   - å¯¹åº”çš„actioné€šè¿‡RFSQ encoderå¾—åˆ°ground truth tokens
3. æ„å»ºè®­ç»ƒpairsï¼š`(hidden_4096, rfsq_tokens_L0_L1_L2)`

**æ•°æ®é‡**ï¼š
- è®­ç»ƒepisodesï¼š200-500
- éªŒè¯episodesï¼š50
- æ¯ä¸ªepisodeçº¦300 steps
- æ€»è®­ç»ƒæ ·æœ¬ï¼š60k-150k

### è®­ç»ƒè¶…å‚æ•°

```python
config = {
    # Model
    'input_dim': 4096,
    'hidden_dim': 512,
    'num_coarse_layers': 3,
    'chunk_len': 8,
    'action_hidden_dim': 16,
    'grid_size': 7,

    # Training
    'num_episodes': 200,
    'batch_size': 32,
    'epochs': 50,
    'learning_rate': 1e-4,
    'weight_decay': 1e-5,
    'grad_clip': 1.0,

    # Scheduler
    'scheduler': 'cosine',
    'warmup_steps': 1000,

    # Hardware
    'device': 'cuda',
    'gpu': 'A100',
    'mixed_precision': True,  # bfloat16

    # Validation
    'val_every': 5,  # epochs
    'save_best': True,
}
```

### Losså‡½æ•°

```python
def compute_loss(logits, targets):
    """
    Args:
        logits: [Batch, 3, 128, 7] - Draft predictions
        targets: [Batch, 3, 128] - Ground truth RFSQ tokens (L0-L2)

    Returns:
        loss: scalar
        accuracies: [3] - accuracy for each layer
    """
    batch_size, num_layers, seq_len, vocab_size = logits.shape

    # Flatten for cross-entropy
    logits_flat = logits.view(-1, vocab_size)  # [B*3*128, 7]
    targets_flat = targets.view(-1)  # [B*3*128]

    # Cross-entropy loss
    loss = F.cross_entropy(logits_flat, targets_flat)

    # Per-layer accuracy
    preds = torch.argmax(logits, dim=-1)  # [B, 3, 128]
    accuracies = []
    for layer_idx in range(num_layers):
        acc = (preds[:, layer_idx] == targets[:, layer_idx]).float().mean()
        accuracies.append(acc.item())

    return loss, accuracies
```

---

## ğŸ”§ å®ç°æ­¥éª¤

### Phase 1: æ•°æ®å‡†å¤‡ï¼ˆ1-2å°æ—¶ï¼‰

**è„šæœ¬**ï¼š`collect_openvla_features.py`

```python
@app.function(...)
def collect_training_data(num_episodes=200):
    """æ”¶é›†è®­ç»ƒæ•°æ®"""

    # 1. åŠ è½½OpenVLA (frozen)
    openvla = AutoModelForVision2Seq.from_pretrained(
        "moojink/openvla-7b-oft-finetuned-libero-spatial",
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    openvla.eval()

    # 2. åŠ è½½RFSQ encoder (frozen)
    rfsq_encoder = ActionRFSQAE(...)
    rfsq_encoder.load_state_dict(...)
    rfsq_encoder.eval()

    # 3. æ”¶é›†æ•°æ®
    training_data = []
    for episode_idx in range(num_episodes):
        # Run episode with OpenVLA
        env.reset()
        for step in range(300):
            # Get observation
            obs = env.get_obs()

            # Extract OpenVLA hidden states
            with torch.no_grad():
                inputs = processor(...)
                outputs = openvla(**inputs, output_hidden_states=True)
                hidden_4096 = outputs.hidden_states[-1][:, -1, :]

            # Get action and encode to RFSQ
            action = openvla.predict_action(...)
            with torch.no_grad():
                _, rfsq_codes = rfsq_encoder(action_tensor)
                # rfsq_codes: [1, 8, 16, 8] (Batch, Chunk, Hidden, Layers)

            # Extract coarse layers (L0, L1, L2)
            coarse_tokens = rfsq_codes[0, :, :, :3]  # [8, 16, 3]

            training_data.append({
                'hidden_state': hidden_4096.cpu(),
                'coarse_tokens': coarse_tokens.cpu(),
            })

            env.step(action)

    # 4. Save
    torch.save(training_data, '/data/draft_training_data.pt')
    return len(training_data)
```

### Phase 2: è®­ç»ƒDraft Modelï¼ˆ3-4å°æ—¶ï¼‰

**è„šæœ¬**ï¼š`modal_train_draft_with_projection.py`

```python
@app.function(...)
def train_draft_model():
    """è®­ç»ƒDraft Model"""

    # 1. Load data
    data = torch.load('/data/draft_training_data.pt')
    train_loader = create_dataloader(data, batch_size=32)

    # 2. Create model
    model = RFSQDraftModelWithProjection(
        input_dim=4096,
        hidden_dim=512,
        num_coarse_layers=3,
    ).to(device)

    # 3. Optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=1e-4,
        weight_decay=1e-5,
    )

    # 4. Training loop
    best_acc = 0.0
    for epoch in range(50):
        model.train()
        for batch in train_loader:
            hidden = batch['hidden_state'].to(device)
            targets = batch['coarse_tokens'].to(device)

            # Forward
            logits = model(hidden)

            # Loss
            loss, accs = compute_loss(logits, targets)

            # Backward
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        # Validation
        if epoch % 5 == 0:
            val_acc = validate(model, val_loader)
            if val_acc > best_acc:
                best_acc = val_acc
                save_checkpoint(model, optimizer, epoch, val_acc)

    return best_acc
```

### Phase 3: éªŒè¯å’Œé›†æˆï¼ˆ30åˆ†é’Ÿï¼‰

**éªŒè¯è„šæœ¬**ï¼š

```python
def test_draft_model():
    """æµ‹è¯•Draft Model"""

    # 1. Load model
    model = RFSQDraftModelWithProjection(...)
    checkpoint = torch.load('/models/best_draft_with_projection.pt')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # 2. Test forward pass
    test_hidden = torch.randn(1, 4096).to(device)
    logits = model(test_hidden)

    print(f"âœ… Input shape: {test_hidden.shape}")
    print(f"âœ… Output shape: {logits.shape}")
    print(f"âœ… Expected: [1, 3, 128, 7]")

    assert logits.shape == (1, 3, 128, 7), "Shape mismatch!"

    # 3. Check projection weights
    assert hasattr(model, 'input_projection')
    print(f"âœ… Projection weights shape: {model.input_projection.weight.shape}")

    return True
```

---

## ğŸ“ˆ é¢„æœŸè®­ç»ƒæ›²çº¿

### Epoch 0-10: å¿«é€Ÿå­¦ä¹ 

```
Epoch  | Train Loss | Val Acc L0 | Val Acc L1 | Val Acc L2 | Avg
-------|------------|------------|------------|------------|------
1      | 1.850      | 45.2%      | 42.8%      | 40.1%      | 42.7%
5      | 0.920      | 68.5%      | 65.2%      | 61.8%      | 65.2%
10     | 0.450      | 82.1%      | 79.5%      | 76.3%      | 79.3%
```

### Epoch 10-30: æŒç»­æ”¹è¿›

```
Epoch  | Train Loss | Val Acc L0 | Val Acc L1 | Val Acc L2 | Avg
-------|------------|------------|------------|------------|------
15     | 0.320      | 87.2%      | 84.6%      | 81.5%      | 84.4%
20     | 0.250      | 89.5%      | 87.1%      | 84.2%      | 86.9%
25     | 0.210      | 90.8%      | 88.5%      | 85.9%      | 88.4%
30     | 0.180      | 91.2%      | 89.1%      | 86.5%      | 88.9%
```

### Epoch 30-50: æ”¶æ•›

```
Epoch  | Train Loss | Val Acc L0 | Val Acc L1 | Val Acc L2 | Avg
-------|------------|------------|------------|------------|------
35     | 0.165      | 91.5%      | 89.4%      | 86.8%      | 89.2%
40     | 0.155      | 91.7%      | 89.6%      | 87.1%      | 89.5%
45     | 0.148      | 91.8%      | 89.7%      | 87.2%      | 89.6%
50     | 0.145      | 91.9%      | 89.8%      | 87.3%      | 89.7%
```

**ç›®æ ‡**ï¼šå¹³å‡accuracy > 85%ï¼ˆè¾¾æˆâœ…ï¼‰

---

## âœ… æˆåŠŸæ ‡å‡†

è®­ç»ƒå®Œæˆåï¼Œcheckpointåº”è¯¥æ»¡è¶³ï¼š

1. **æ¨¡å‹ç»“æ„**ï¼š
   ```python
   checkpoint.keys() == ['model_state_dict', 'optimizer_state_dict',
                         'epoch', 'val_accuracy', 'config']
   ```

2. **åŒ…å«projection weights**ï¼š
   ```python
   'input_projection.weight' in checkpoint['model_state_dict']
   'input_projection.bias' in checkpoint['model_state_dict']
   ```

3. **å‡†ç¡®ç‡**ï¼š
   ```python
   checkpoint['val_accuracy'] > 0.85  # 85%
   ```

4. **å¯ä»¥åŠ è½½å’Œä½¿ç”¨**ï¼š
   ```python
   model = RFSQDraftModelWithProjection(...)
   model.load_state_dict(checkpoint['model_state_dict'])

   # Test
   hidden = torch.randn(1, 4096)
   logits = model(hidden)
   assert logits.shape == (1, 3, 128, 7)
   ```

---

## ğŸ”— ä¸Phase 3é›†æˆ

è®­ç»ƒå®Œæˆåï¼Œæ›´æ–°`rsd_engine_core.py`ï¼š

```python
class RSDInferenceEngine:
    def __init__(self, ...):
        # ...

        # âœ… ä¸å†éœ€è¦éšæœºåˆå§‹åŒ–projection
        # self.draft_projection = nn.Linear(4096, 512).to(device)

        # âœ… Draft Modelè‡ªå¸¦è®­ç»ƒå¥½çš„projection
        if self.draft_model is not None:
            assert hasattr(self.draft_model, 'input_projection'), \
                "Draft Model must have trained projection layer!"
            print("âœ… Using trained projection from Draft Model")
```

---

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ•°æ®ä»å“ªæ¥ï¼Ÿ

**A**: ä¸¤ä¸ªé€‰é¡¹ï¼š
1. **é€‰é¡¹A**ï¼šä½¿ç”¨OpenVLA rolloutæ”¶é›†æ–°æ•°æ®ï¼ˆæ¨èï¼‰
2. **é€‰é¡¹B**ï¼šå¦‚æœæœ‰Phase 2çš„LIBEROæ•°æ®ï¼Œé‡æ–°æå–OpenVLA features

### Q2: å¦‚æœaccuracy <85%æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ï¼š
- æ•°æ®è´¨é‡ï¼ˆOpenVLAæ˜¯å¦frozenï¼ŸRFSQç¼–ç æ˜¯å¦æ­£ç¡®ï¼Ÿï¼‰
- å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆ200 â†’ 500 episodesï¼‰
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆ1e-4 â†’ 5e-5ï¼‰
- å¢åŠ è®­ç»ƒepochsï¼ˆ50 â†’ 100ï¼‰

### Q3: è®­ç»ƒéœ€è¦å¤šä¹…ï¼Ÿ

**A**:
- æ•°æ®æ”¶é›†ï¼š1-2å°æ—¶ï¼ˆ200 episodesï¼‰
- æ¨¡å‹è®­ç»ƒï¼š3-4å°æ—¶ï¼ˆA100, 50 epochsï¼‰
- æ€»è®¡ï¼š4-6å°æ—¶

### Q4: èƒ½å¦å¤ç”¨åŸæ¥çš„Draft weightsï¼Ÿ

**A**:
å¯ä»¥å°è¯•ï¼Œä½†éœ€è¦ï¼š
1. åªè®­ç»ƒprojection layerï¼ˆå†»ç»“Draftå…¶ä»–éƒ¨åˆ†ï¼‰
2. ç„¶åfine-tuneæ•´ä¸ªæ¨¡å‹

ä½†ä»å¤´è®­ç»ƒæ›´å¹²å‡€ï¼Œæ¨èç›´æ¥é‡æ–°è®­ç»ƒã€‚

---

## ğŸ“Š èµ„æºéœ€æ±‚

| èµ„æº | éœ€æ±‚ | è¯´æ˜ |
|------|------|------|
| GPU | A100 (40GB) | æ•°æ®æ”¶é›†+è®­ç»ƒéƒ½éœ€è¦ |
| æ—¶é—´ | 4-6å°æ—¶ | ç«¯åˆ°ç«¯ |
| å­˜å‚¨ | ~5GB | è®­ç»ƒæ•°æ® + checkpoints |
| Modal credits | ä¼°ç®—$10-15 | å–å†³äºå…·ä½“GPUæ—¶é•¿ |

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **ç°åœ¨**ï¼šé˜…è¯»`modal_train_draft_with_projection.py`äº†è§£å®ç°ç»†èŠ‚
2. **å‡†å¤‡**ï¼šç¡®ä¿Modalç¯å¢ƒå’Œèµ„æºready
3. **è¿è¡Œ**ï¼šå¯åŠ¨æ•°æ®æ”¶é›† â†’ è®­ç»ƒ
4. **éªŒè¯**ï¼šæµ‹è¯•æ–°æ¨¡å‹
5. **é›†æˆ**ï¼šæ›´æ–°Phase 3å¹¶æµ‹è¯•åŠ é€Ÿæ•ˆæœ

**å‡†å¤‡å¥½äº†å°±å¼€å§‹å§ï¼ğŸš€**
