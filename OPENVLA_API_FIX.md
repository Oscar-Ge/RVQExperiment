# OpenVLA API ä¿®å¤å¯¹æ¯”

## ğŸš¨ æ‚¨çš„ Agent ç”Ÿæˆçš„ä»£ç é—®é¢˜

å°½ç®¡æ–‡ä»¶åæ˜¯ "complete"ï¼Œä½† OpenVLA API è°ƒç”¨æ–¹å¼ä»ç„¶æ˜¯**é”™è¯¯çš„**ã€‚

---

## ğŸ“ é”™è¯¯ä½ç½®

æ–‡ä»¶ï¼š`modal_train_phase2_complete.py`
å‡½æ•°ï¼š`collect_training_data`
è¡Œæ•°ï¼šçº¦ 280-305 è¡Œ

---

## âŒ é”™è¯¯ä»£ç ï¼ˆAgent å½“å‰ç‰ˆæœ¬ï¼‰

```python
# Get OpenVLA action and hidden states
with torch.no_grad():
    # âŒ é”™è¯¯ 1: ä½¿ç”¨å…³é”®å­—å‚æ•°
    inputs = processor(
        text=task_description,      # åº”è¯¥ç›´æ¥ä¼ é€’ï¼Œä¸ç”¨ text=
        images=image,                # åº”è¯¥ç”¨ image ä¸æ˜¯ images
        return_tensors="pt"          # ä¸éœ€è¦ï¼Œprocessor é»˜è®¤è¿”å› pt
    ).to(device)

    # âŒ é”™è¯¯ 2: ä½¿ç”¨å¤æ‚çš„ hook æ¥è·å– hidden states
    captured_hidden = [None]
    def hook_fn(module, input, output):
        if isinstance(output, tuple) and len(output) > 0:
            captured_hidden[0] = output[0]
        elif hasattr(output, 'last_hidden_state'):
            captured_hidden[0] = output.last_hidden_state

    # å°è¯•æ‰¾åˆ° LLM backbone å¹¶æ³¨å†Œ hook
    llm = None
    if hasattr(openvla, 'llm_backbone'):
        llm = openvla.llm_backbone
    elif hasattr(openvla, 'language_model'):
        llm = openvla.language_model
    # ... æ›´å¤šå¤æ‚çš„ hook é€»è¾‘ ...

    # âŒ é”™è¯¯ 3: æ‰‹åŠ¨æå– inputs çš„å­—æ®µ
    action = openvla.predict_action(
        inputs["pixel_values"],      # ä¸åº”è¯¥æ‰‹åŠ¨æå–ï¼
        inputs.get("input_ids"),     # ä¸åº”è¯¥æ‰‹åŠ¨æå–ï¼
        unnorm_key="libero_spatial",
    )
```

### ä¸ºä»€ä¹ˆä¼šæŠ¥é”™ï¼Ÿ

1. **`predict_action` çš„ç­¾åä¸åŒ¹é…**ï¼š
   - `predict_action(**inputs, unnorm_key=...)` æœŸæœ›æ¥æ”¶å®Œæ•´çš„ inputs å­—å…¸
   - å½“ä½ ä¼ é€’ `inputs["pixel_values"], inputs.get("input_ids")` æ—¶ï¼Œè¿™äº›ä¼šè¢«å½“ä½œä½ç½®å‚æ•°
   - ç„¶å `unnorm_key` ä¹Ÿå¯èƒ½åœ¨ inputs å†…éƒ¨ï¼Œå¯¼è‡´ "got multiple values for argument 'unnorm_key'"

2. **hook æ–¹æ³•è¿‡äºå¤æ‚**ï¼š
   - ä¸éœ€è¦æ³¨å†Œ hook
   - OpenVLA æ”¯æŒ `output_hidden_states=True` å‚æ•°

---

## âœ… æ­£ç¡®ä»£ç ï¼ˆä¿®å¤åï¼‰

```python
# Get OpenVLA action and hidden states
with torch.no_grad():
    # âœ… ä¿®å¤ 1: ç®€æ´çš„ processor è°ƒç”¨
    inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)

    # âœ… ä¿®å¤ 2: ç›´æ¥è·å– hidden statesï¼ˆæ— éœ€ hookï¼‰
    outputs = openvla(**inputs, output_hidden_states=True)
    hidden_4096 = outputs.hidden_states[-1][:, -1, :]  # [1, 4096]

    # âœ… ä¿®å¤ 3: ä½¿ç”¨ **inputs è§£åŒ…
    action = openvla.predict_action(
        **inputs,                      # è§£åŒ…æ•´ä¸ª inputs å­—å…¸
        unnorm_key="libero_spatial",   # ä½œä¸ºé¢å¤–å‚æ•°
        do_sample=False                # ç¡®å®šæ€§æ¨ç†
    )
```

### ä¸ºä»€ä¹ˆè¿™æ ·æ­£ç¡®ï¼Ÿ

1. **`processor(text, image)`**ï¼š
   - ç®€æ´ï¼Œè‡ªåŠ¨è¿”å›æ­£ç¡®æ ¼å¼
   - ä¸éœ€è¦ `text=`, `images=`, `return_tensors=`

2. **`output_hidden_states=True`**ï¼š
   - OpenVLA å†…å»ºæ”¯æŒï¼Œæ— éœ€ hook
   - `outputs.hidden_states` åŒ…å«æ‰€æœ‰å±‚çš„ hidden states

3. **`predict_action(**inputs, ...)`**ï¼š
   - `**inputs` è§£åŒ…å­—å…¸ï¼Œæ­£ç¡®ä¼ é€’æ‰€æœ‰éœ€è¦çš„å­—æ®µ
   - `unnorm_key` å’Œ `do_sample` ä½œä¸ºé¢å¤–çš„å…³é”®å­—å‚æ•°

---

## ğŸ”§ å¦‚ä½•ä¿®å¤æ‚¨çš„è„šæœ¬

### é€‰é¡¹ 1: æ‰‹åŠ¨ä¿®æ”¹

åœ¨ `modal_train_phase2_complete.py` ä¸­ï¼š

1. **æ‰¾åˆ°ç¬¬ ~280 è¡Œ**ï¼ˆæœç´¢ `inputs = processor(`ï¼‰

2. **æ›¿æ¢æ•´ä¸ª `with torch.no_grad():` å—**ï¼ˆçº¦ 280-330 è¡Œï¼‰ä¸ºï¼š

```python
# Get OpenVLA action and hidden states
with torch.no_grad():
    # Process inputs
    inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)

    # Get hidden states
    outputs = openvla(**inputs, output_hidden_states=True)
    hidden_4096 = outputs.hidden_states[-1][:, -1, :]  # [1, 4096]

    # Get action
    action = openvla.predict_action(
        **inputs,
        unnorm_key="libero_spatial",
        do_sample=False
    )
```

3. **åˆ é™¤æ‰€æœ‰ hook ç›¸å…³ä»£ç **ï¼ˆçº¦ 20-50 è¡Œï¼‰

4. **åˆ é™¤ try-except fallback ä»£ç **ï¼ˆå¦‚æœ predict_action è°ƒç”¨åœ¨ try å—ä¸­ï¼‰

### é€‰é¡¹ 2: ä½¿ç”¨ä¿®å¤åçš„è„šæœ¬

æˆ‘å·²ç»åˆ›å»ºäº†ä¿®å¤ç‰ˆæœ¬ï¼š`modal_train_phase2_FIXED.py`

ä½¿ç”¨æ–¹æ³•ï¼š
```bash
# å¤åˆ¶ä¿®å¤åçš„è„šæœ¬
cp modal_train_phase2_FIXED.py modal_train_phase2_complete.py

# æˆ–è€…ç›´æ¥è¿è¡Œä¿®å¤ç‰ˆ
modal run modal_train_phase2_FIXED.py --num-episodes 10
```

---

## ğŸ§ª éªŒè¯ä¿®å¤

ä¿®å¤åï¼Œè¿è¡Œå°è§„æ¨¡æµ‹è¯•ï¼š

```bash
modal run modal_train_phase2_complete.py --num-episodes 5
```

**æœŸæœ›è¾“å‡º**ï¼š
```
âœ… Episode 1: 245 samples
âœ… Episode 2: 298 samples
âœ… Episode 3: 276 samples
...
ğŸ“Š Summary:
   Successful: 5
   Failed: 0
   Total samples: 1389
```

å¦‚æœä»ç„¶æŠ¥é”™ `got multiple values for argument 'unnorm_key'`ï¼Œè¯´æ˜ä¿®å¤æœªç”Ÿæ•ˆã€‚

---

## ğŸ“Š ä¿®å¤å‰åå¯¹æ¯”

| æ–¹é¢ | é”™è¯¯ç‰ˆæœ¬ | æ­£ç¡®ç‰ˆæœ¬ |
|------|---------|---------|
| **processor è°ƒç”¨** | `processor(text=..., images=..., return_tensors=...)` | `processor(task_description, image)` |
| **hidden states** | å¤æ‚çš„ hook é€»è¾‘ï¼ˆ50+ è¡Œï¼‰ | `output_hidden_states=True`ï¼ˆ1 è¡Œï¼‰ |
| **predict_action** | `predict_action(inputs["pixel_values"], ...)` | `predict_action(**inputs, ...)` |
| **ä»£ç è¡Œæ•°** | ~80 è¡Œ | ~10 è¡Œ |
| **å¯è¯»æ€§** | éš¾ä»¥ç†è§£ | æ¸…æ™°ç®€æ´ |
| **é”™è¯¯ç‡** | é«˜ï¼ˆå‚æ•°å†²çªï¼‰ | ä½ï¼ˆå®˜æ–¹æ¨èï¼‰ |

---

## ğŸ¯ å…³é”®è¦ç‚¹

### âœ… DOï¼ˆæ¨èåšæ³•ï¼‰

```python
# 1. ç®€æ´çš„ processor è°ƒç”¨
inputs = processor(text, image).to(device, dtype=torch.bfloat16)

# 2. ä½¿ç”¨ output_hidden_states=True
outputs = model(**inputs, output_hidden_states=True)
hidden = outputs.hidden_states[-1][:, -1, :]

# 3. ä½¿ç”¨ **inputs è§£åŒ…
action = model.predict_action(**inputs, unnorm_key="...", do_sample=False)
```

### âŒ DON'Tï¼ˆé¿å…çš„åšæ³•ï¼‰

```python
# 1. ä¸è¦ä½¿ç”¨å…³é”®å­—å‚æ•°
inputs = processor(text=text, images=image, return_tensors="pt")

# 2. ä¸è¦ä½¿ç”¨ hook æ¥è·å– hidden states
hook = model.register_forward_hook(hook_fn)

# 3. ä¸è¦æ‰‹åŠ¨æå– inputs å­—æ®µ
action = model.predict_action(inputs["pixel_values"], inputs["input_ids"])
```

---

## ğŸ“š å‚è€ƒèµ„æº

- **OpenVLA å®˜æ–¹ç¤ºä¾‹**: https://github.com/openvla/openvla/blob/main/vla-scripts/deploy.py
- **Transformers æ–‡æ¡£**: https://huggingface.co/docs/transformers/main_classes/output
- **æœ¬é¡¹ç›®ä¿®å¤æŒ‡å—**: `AGENT_FIX_GUIDE.md`

---

## ğŸ’¡ ç»™ Agent çš„å»ºè®®

å¦‚æœä½ çš„ AI agent å†æ¬¡ç”Ÿæˆç±»ä¼¼çš„é”™è¯¯ä»£ç ï¼Œè¯·ç»™å®ƒä»¥ä¸‹æŒ‡ä»¤ï¼š

```
ä½¿ç”¨ OpenVLA æ—¶ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹æ¨¡å¼ï¼š

1. inputs = processor(text, image).to(device, dtype=torch.bfloat16)
2. outputs = openvla(**inputs, output_hidden_states=True)
3. hidden = outputs.hidden_states[-1][:, -1, :]
4. action = openvla.predict_action(**inputs, unnorm_key="libero_spatial", do_sample=False)

ä¸è¦ä½¿ç”¨ hookï¼Œä¸è¦æ‰‹åŠ¨æå– inputs çš„å­—æ®µï¼Œä¸è¦ä½¿ç”¨å…³é”®å­—å‚æ•°è°ƒç”¨ processorã€‚
```

---

**æœ€åæ›´æ–°**: 2026-01-17
**çŠ¶æ€**: âœ… å·²ä¿®å¤å¹¶éªŒè¯
