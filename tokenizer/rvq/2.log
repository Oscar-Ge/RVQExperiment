[robosuite WARNING] No private macro file found! (__init__.py:7)
WARNING:robosuite_logs:No private macro file found!
[robosuite WARNING] It is recommended to use a private macro file (__init__.py:8)
WARNING:robosuite_logs:It is recommended to use a private macro file
[robosuite WARNING] To setup, run: python /venv/main/lib/python3.12/site-packages/robosuite/scripts/setup_macros.py (__init__.py:9)
WARNING:robosuite_logs:To setup, run: python /venv/main/lib/python3.12/site-packages/robosuite/scripts/setup_macros.py
Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.
Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.
Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.
See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.
================================================================================
RVQ TOKENIZER TRAINING
================================================================================
================================================================================
COLLECTING œÄ0.5 ACTIONS ON LIBERO
================================================================================

[1/4] Loading œÄ0.5 policy...
‚úì PyTorch checkpoint already exists: /root/.cache/openpi/converted_checkpoints/pi05_libero_pytorch
  Use --force_conversion to re-convert

Loading pi05-libero policy from: /root/.cache/openpi/converted_checkpoints/pi05_libero_pytorch
Creating policy with pi05_libero configuration...
/venv/main/lib/python3.12/site-packages/torch/jit/_script.py:365: DeprecationWarning: `torch.jit.script_method` is deprecated. Please switch to `torch.compile` or `torch.export`.
  warnings.warn(
‚úì Policy loaded successfully!

GPU Memory Usage:
  Allocated: 7.49 GB
  Reserved: 7.52 GB

[2/4] Loading LIBERO task: libero_spatial - Task 0
[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
[Warning]: datasets path /workspace/basic-run/LIBERO/libero/libero/../datasets does not exist!
  Task: pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate
  Description: pick up the black bowl between the plate and the ramekin and place it on the plate
  Initial states: 50

[3/4] Creating LIBERO environment...
[Warning]: datasets path /workspace/basic-run/LIBERO/libero/libero/../datasets does not exist!

[4/4] Collecting actions from 50 episodes...
/venv/main/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py:2033: UserWarning: Dynamo does not know how to trace the builtin `builtins.__build_class__.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).
If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.
If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.
  torch._dynamo.utils.warn_once(explanation + "\n" + "\n".join(hints))

  [DEBUG] First action_chunk shape: (10, 7)
  [DEBUG] action_chunk dtype: float64
  [DEBUG] action_chunk type: <class 'numpy.ndarray'>
  Episode 1/50: ‚úÖ (steps: 88)
  Episode 2/50: ‚úÖ (steps: 140)
  Episode 3/50: ‚úÖ (steps: 89)
  Episode 4/50: ‚úÖ (steps: 88)
  Episode 5/50: ‚úÖ (steps: 83)
  Episode 6/50: ‚úÖ (steps: 86)
  Episode 7/50: ‚úÖ (steps: 81)
  Episode 8/50: ‚úÖ (steps: 94)
  Episode 9/50: ‚úÖ (steps: 96)
  Episode 10/50: ‚úÖ (steps: 88)
  Episode 11/50: ‚úÖ (steps: 86)
  Episode 12/50: ‚úÖ (steps: 86)
  Episode 13/50: ‚úÖ (steps: 91)
  Episode 14/50: ‚úÖ (steps: 143)
  Episode 15/50: ‚úÖ (steps: 96)
  Episode 16/50: ‚úÖ (steps: 83)
  Episode 17/50: ‚úÖ (steps: 86)
  Episode 18/50: ‚úÖ (steps: 84)
  Episode 19/50: ‚úÖ (steps: 106)
  Episode 20/50: ‚úÖ (steps: 87)
  Episode 21/50: ‚úÖ (steps: 91)
  Episode 22/50: ‚úÖ (steps: 79)
  Episode 23/50: ‚úÖ (steps: 87)
  Episode 24/50: ‚úÖ (steps: 85)
  Episode 25/50: ‚úÖ (steps: 83)
  Episode 26/50: ‚úÖ (steps: 82)
  Episode 27/50: ‚úÖ (steps: 140)
  Episode 28/50: ‚úÖ (steps: 84)
  Episode 29/50: ‚úÖ (steps: 80)
  Episode 30/50: ‚úÖ (steps: 82)
  Episode 31/50: ‚úÖ (steps: 88)
  Episode 32/50: ‚úÖ (steps: 89)
  Episode 33/50: ‚úÖ (steps: 90)
  Episode 34/50: ‚úÖ (steps: 83)
  Episode 35/50: ‚úÖ (steps: 82)
  Episode 36/50: ‚úÖ (steps: 135)
  Episode 37/50: ‚úÖ (steps: 81)
  Episode 38/50: ‚úÖ (steps: 89)
  Episode 39/50: ‚úÖ (steps: 85)
  Episode 40/50: ‚ùå (steps: 220)
  Episode 41/50: ‚úÖ (steps: 83)
  Episode 42/50: ‚úÖ (steps: 82)
  Episode 43/50: ‚úÖ (steps: 87)
  Episode 44/50: ‚úÖ (steps: 83)
  Episode 45/50: ‚úÖ (steps: 81)
  Episode 46/50: ‚úÖ (steps: 85)
  Episode 47/50: ‚úÖ (steps: 90)
  Episode 48/50: ‚úÖ (steps: 90)
  Episode 49/50: ‚úÖ (steps: 87)
  Episode 50/50: ‚úÖ (steps: 83)

‚úÖ Collected 4167 action chunks
   Success rate: 98.0%
   Avg episode length: 93.3 steps
================================================================================
TRAINING RVQ TOKENIZER
================================================================================

Dataset:
  Action chunks: 4167
  Chunk size: 10
  Action dim: 7

Model config:
  Num layers: 8
  Hidden dim: 64
  Codebook size: 256
  Residual dropout: 0.1

Training config:
  Epochs: 100
  Batch size: 128
  Learning rate: 0.001
  Device: cuda
Fitted RVQ tokenizer on 41670 actions
  Action range: [-1.0165, 1.0145]

================================================================================
TRAINING PROGRESS
================================================================================
Epoch 10/100:
  Recon Loss: 0.002426
  VQ Loss: 0.012356
  Total Loss: 0.014782
Epoch 20/100:
  Recon Loss: 0.000511
  VQ Loss: 0.003387
  Total Loss: 0.003899
Epoch 30/100:
  Recon Loss: 0.000480
  VQ Loss: 0.001787
  Total Loss: 0.002267
Epoch 40/100:
  Recon Loss: 0.000422
  VQ Loss: 0.001285
  Total Loss: 0.001707
Epoch 50/100:
  Recon Loss: 0.000401
  VQ Loss: 0.001134
  Total Loss: 0.001536
Epoch 60/100:
  Recon Loss: 0.000468
  VQ Loss: 0.000920
  Total Loss: 0.001388
Epoch 70/100:
  Recon Loss: 0.000399
  VQ Loss: 0.000829
  Total Loss: 0.001228
Epoch 80/100:
  Recon Loss: 0.000377
  VQ Loss: 0.000763
  Total Loss: 0.001141
Epoch 90/100:
  Recon Loss: 0.000288
  VQ Loss: 0.000818
  Total Loss: 0.001106
Epoch 100/100:
  Recon Loss: 0.000212
  VQ Loss: 0.000809
  Total Loss: 0.001020

‚úÖ Model saved to rvq_tokenizer.pt

üìä Training curves saved to training_history.png

================================================================================
QUICK VALIDATION
================================================================================

Test reconstruction MSE: 0.000113
‚úÖ Validation passed! MSE < 0.01

================================================================================
NEXT STEPS
================================================================================
‚úÖ RVQ tokenizer trained successfully!

Model saved to: rvq_tokenizer.pt

Next:
  1. Run: python analyze_rvq_compression.py --model rvq_tokenizer.pt
  2. Test different numbers of layers (1-8)
  3. Plot MSE vs. number of layers
================================================================================

sys:1: DeprecationWarning: builtin type swigvarlink has no __module__ attribute
