# Agent ä¿®å¤æŒ‡å—ï¼šOpenVLA API è°ƒç”¨é”™è¯¯

## ğŸš¨ é—®é¢˜æ€»ç»“

ä½ é‡åˆ°çš„ä¸¤ä¸ªé”™è¯¯ï¼š
1. `TypeError: got multiple values for argument 'unnorm_key'`
2. `ValueError: num_samples=0`

**æ ¹æœ¬åŸå› **ï¼šOpenVLA çš„ `predict_action` API è°ƒç”¨æ–¹å¼é”™è¯¯ï¼Œå¯¼è‡´æ•°æ®æ”¶é›†å¤±è´¥ã€‚

---

## âœ… æ­£ç¡®çš„ OpenVLA API è°ƒç”¨æ–¹å¼

### é”™è¯¯ç¤ºä¾‹ï¼ˆä½ å½“å‰çš„ä»£ç ï¼‰

```python
# âŒ è¿™æ˜¯é”™è¯¯çš„ï¼
action = openvla.predict_action(
    image,                        # ä¸è¦ç›´æ¥ä¼  image
    task_description,             # ä¸è¦ç›´æ¥ä¼  task_description
    unnorm_key="libero_spatial",  # è¿™ä¼šå¯¼è‡´å‚æ•°å†²çª
)
```

### æ­£ç¡®ç¤ºä¾‹

```python
# âœ… è¿™æ˜¯æ­£ç¡®çš„ï¼
# Step 1: ä½¿ç”¨ processor å¤„ç†è¾“å…¥
inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)

# Step 2: ä½¿ç”¨ **inputs è§£åŒ…ä¼ é€’
action = openvla.predict_action(**inputs, unnorm_key="libero_spatial", do_sample=False)
```

---

## ğŸ“ å®Œæ•´çš„æ•°æ®æ”¶é›†ä»£ç ä¿®å¤

### ä¿®å¤å‰ï¼ˆé”™è¯¯ç‰ˆæœ¬ï¼‰

```python
# Get OpenVLA hidden states
with torch.no_grad():
    inputs = processor(
        text=task_description,
        images=image,
        return_tensors="pt"
    ).to(device)

    outputs = openvla(**inputs, output_hidden_states=True)
    hidden_4096 = outputs.hidden_states[-1][:, -1, :]  # [1, 4096]

    # âŒ é”™è¯¯çš„ predict_action è°ƒç”¨
    action = openvla.predict_action(
        image,
        task_description,
        unnorm_key="libero_spatial",
    )
```

### ä¿®å¤åï¼ˆæ­£ç¡®ç‰ˆæœ¬ï¼‰

```python
# Get OpenVLA hidden states and action
with torch.no_grad():
    # Step 1: å¤„ç†è¾“å…¥ï¼ˆæ³¨æ„å‚æ•°é¡ºåºï¼štext åœ¨å‰ï¼Œimages åœ¨åï¼‰
    inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)

    # Step 2: è·å– hidden statesï¼ˆéœ€è¦ä¼ å…¥ output_hidden_states=Trueï¼‰
    outputs = openvla(**inputs, output_hidden_states=True)
    hidden_4096 = outputs.hidden_states[-1][:, -1, :]  # [1, 4096]

    # Step 3: æ­£ç¡®è°ƒç”¨ predict_actionï¼ˆé‡ç”¨ inputsï¼‰
    action = openvla.predict_action(**inputs, unnorm_key="libero_spatial", do_sample=False)
```

**å…³é”®ç‚¹**ï¼š
1. `processor()` çš„å‚æ•°é¡ºåºæ˜¯ `(text, image)`ï¼Œä¸æ˜¯ `(text=..., images=...)`
2. ä½¿ç”¨ `**inputs` è§£åŒ…ä¼ é€’ç»™ `predict_action`
3. `unnorm_key` å’Œ `do_sample` ä½œä¸ºé¢å¤–çš„å…³é”®å­—å‚æ•°ä¼ é€’

---

## ğŸ”§ å®Œæ•´çš„ä¿®å¤æ¸…å•

### 1. æ‰¾åˆ°æ•°æ®æ”¶é›†å‡½æ•°

åœ¨ä½ çš„è„šæœ¬ä¸­æ‰¾åˆ° `collect_training_data` å‡½æ•°ï¼ˆæˆ–ç±»ä¼¼åç§°ï¼‰ã€‚

### 2. å®šä½ OpenVLA è°ƒç”¨ä»£ç 

æœç´¢åŒ…å« `openvla.predict_action` çš„ä»£ç å—ã€‚

### 3. åº”ç”¨ä¿®å¤

å°†ä»¥ä¸‹é”™è¯¯æ¨¡å¼ï¼š

```python
# âŒ æŸ¥æ‰¾å¹¶åˆ é™¤è¿™ç§æ¨¡å¼
inputs = processor(
    text=task_description,
    images=image,
    return_tensors="pt"
).to(device)

outputs = openvla(**inputs, output_hidden_states=True)
hidden_4096 = outputs.hidden_states[-1][:, -1, :]

action = openvla.predict_action(
    image,
    task_description,
    unnorm_key="libero_spatial",
)
```

æ›¿æ¢ä¸ºï¼š

```python
# âœ… ä½¿ç”¨è¿™ç§æ­£ç¡®çš„æ¨¡å¼
inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)

outputs = openvla(**inputs, output_hidden_states=True)
hidden_4096 = outputs.hidden_states[-1][:, -1, :]

action = openvla.predict_action(**inputs, unnorm_key="libero_spatial", do_sample=False)
```

### 4. æ·»åŠ é”™è¯¯å¤„ç†

ç¡®ä¿æœ‰é€‚å½“çš„é”™è¯¯å¤„ç†ï¼Œé¿å…æ‰€æœ‰ episodes éƒ½å¤±è´¥ï¼š

```python
for episode_idx in range(episodes_per_task):
    try:
        # ... ç¯å¢ƒè®¾ç½®ä»£ç  ...

        for step in range(300):
            try:
                # æ­£ç¡®çš„ OpenVLA è°ƒç”¨
                inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)
                outputs = openvla(**inputs, output_hidden_states=True)
                hidden_4096 = outputs.hidden_states[-1][:, -1, :]
                action = openvla.predict_action(**inputs, unnorm_key="libero_spatial", do_sample=False)

                # ... å…¶ä½™ä»£ç  ...

            except Exception as step_error:
                print(f"        âš ï¸ Step {step} failed: {step_error}")
                # ç»§ç»­ä¸‹ä¸€ä¸ª stepï¼Œè€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ª episode
                continue

        print(f"      âœ… Episode {episode_idx + 1}: collected {len(episode_data)} samples")

    except Exception as episode_error:
        print(f"      âš ï¸ Episode {episode_idx + 1} failed: {episode_error}")
        # ç»§ç»­ä¸‹ä¸€ä¸ª episode
        continue
```

### 5. éªŒè¯æ•°æ®æ”¶é›†

åœ¨æ•°æ®æ”¶é›†åæ·»åŠ éªŒè¯ï¼š

```python
# 5. éªŒè¯å¹¶ä¿å­˜æ•°æ®
print(f"\n5ï¸âƒ£ Data collection summary:")
print(f"   Total samples collected: {len(training_data)}")

if len(training_data) == 0:
    raise ValueError("âŒ No training data collected! Check OpenVLA API calls and error logs.")

if len(training_data) < 1000:
    print(f"   âš ï¸ Warning: Only {len(training_data)} samples collected. Consider:")
    print(f"      - Increasing num_episodes")
    print(f"      - Checking episode success rate")
    print(f"      - Reviewing error logs above")

print(f"\n6ï¸âƒ£ Saving {len(training_data)} samples...")
save_path = "/data/draft_training_data.pt"
torch.save(training_data, save_path)
print(f"   âœ… Saved to {save_path}")
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹ï¼šä¿®å¤åçš„ collect_training_data å‡½æ•°

```python
@app.function(...)
def collect_training_data(num_episodes: int = 200):
    """æ”¶é›†è®­ç»ƒæ•°æ®ï¼šOpenVLA hidden states + RFSQ token labels"""

    # ... åˆå§‹åŒ–ä»£ç  ...

    # åŠ è½½æ¨¡å‹
    print("\n1ï¸âƒ£ Loading OpenVLA (frozen)...")
    openvla = AutoModelForVision2Seq.from_pretrained(
        "moojink/openvla-7b-oft-finetuned-libero-spatial",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    processor = AutoProcessor.from_pretrained(
        "moojink/openvla-7b-oft-finetuned-libero-spatial",
        trust_remote_code=True,
    )
    openvla.eval()

    # ... åŠ è½½ RFSQ encoder ...

    # æ”¶é›†æ•°æ®
    training_data = []
    successful_episodes = 0
    failed_episodes = 0

    for task_id in range(num_tasks):
        task = task_suite.get_task(task_id)
        task_description = task.language

        for episode_idx in range(episodes_per_task):
            try:
                # åˆ›å»ºç¯å¢ƒ
                env = OffScreenRenderEnv(...)
                env.reset()
                obs = env.set_init_state(init_states[episode_idx])

                episode_samples = []

                # Episode loop
                for step in range(300):
                    try:
                        # å‡†å¤‡å›¾åƒ
                        image = Image.fromarray(obs['agentview_image'].astype(np.uint8))

                        # âœ… æ­£ç¡®çš„ OpenVLA è°ƒç”¨
                        with torch.no_grad():
                            # Step 1: å¤„ç†è¾“å…¥
                            inputs = processor(task_description, image).to(device, dtype=torch.bfloat16)

                            # Step 2: è·å– hidden states
                            outputs = openvla(**inputs, output_hidden_states=True)
                            hidden_4096 = outputs.hidden_states[-1][:, -1, :]  # [1, 4096]

                            # Step 3: è·å– action
                            action = openvla.predict_action(
                                **inputs,
                                unnorm_key="libero_spatial",
                                do_sample=False
                            )

                        # ç¼–ç  action åˆ° RFSQ tokens
                        with torch.no_grad():
                            action_tensor = torch.from_numpy(action).float().unsqueeze(0).to(device)
                            action_chunk = action_tensor.unsqueeze(1).expand(1, 8, 7)
                            _, rfsq_codes = rfsq_encoder(action_chunk)
                            coarse_tokens = rfsq_codes[0, :, :, :3]

                        # ä¿å­˜æ ·æœ¬
                        episode_samples.append({
                            'hidden_state': hidden_4096.squeeze(0).cpu(),
                            'coarse_tokens': coarse_tokens.cpu(),
                        })

                        # Step environment
                        obs, reward, done, info = env.step(action)
                        if done:
                            break

                    except Exception as step_error:
                        print(f"        âš ï¸ Step {step} failed: {step_error}")
                        continue

                # Episode å®Œæˆ
                env.close()
                if len(episode_samples) > 0:
                    training_data.extend(episode_samples)
                    successful_episodes += 1
                    print(f"      âœ… Episode {episode_idx + 1}: {len(episode_samples)} samples")
                else:
                    failed_episodes += 1
                    print(f"      âš ï¸ Episode {episode_idx + 1}: No samples collected")

            except Exception as episode_error:
                failed_episodes += 1
                print(f"      âš ï¸ Episode {episode_idx + 1} failed: {episode_error}")
                continue

    # æ€»ç»“
    print(f"\nğŸ“Š Collection Summary:")
    print(f"   Successful episodes: {successful_episodes}")
    print(f"   Failed episodes: {failed_episodes}")
    print(f"   Total samples: {len(training_data)}")

    # éªŒè¯
    if len(training_data) == 0:
        raise ValueError("âŒ No training data collected! All episodes failed. Check error logs.")

    # ä¿å­˜
    save_path = "/data/draft_training_data.pt"
    torch.save(training_data, save_path)
    data_volume.commit()

    return len(training_data)
```

---

## ğŸ§ª æµ‹è¯•ä¿®å¤

è¿è¡Œä¿®å¤åçš„è„šæœ¬ï¼š

```bash
modal run your_fixed_script.py --num-episodes 10  # å…ˆæµ‹è¯•å°‘é‡ episodes
```

**æœŸæœ›è¾“å‡º**ï¼š
```
âœ… Episode 1: 245 samples
âœ… Episode 2: 298 samples
...
ğŸ“Š Collection Summary:
   Successful episodes: 10
   Failed episodes: 0
   Total samples: 2547
```

å¦‚æœä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥ï¼š
1. âœ… `processor()` å‚æ•°é¡ºåºï¼š`(text, image)` ä¸æ˜¯ `(text=..., images=...)`
2. âœ… `predict_action()` ä½¿ç”¨ `**inputs` è§£åŒ…
3. âœ… `unnorm_key` ä½œä¸ºé¢å¤–å‚æ•°ä¼ é€’
4. âœ… æ·»åŠ äº†é€‚å½“çš„é”™è¯¯å¤„ç†

---

## ğŸ“š å‚è€ƒèµ„æº

- OpenVLA å®˜æ–¹ç¤ºä¾‹: https://github.com/openvla/openvla/blob/main/vla-scripts/deploy.py
- Hugging Face æ–‡æ¡£: https://huggingface.co/openvla/openvla-7b

---

**æ€»ç»“**ï¼šOpenVLA çš„ API ä¸æ¥å—ç›´æ¥ä¼ é€’ `image` å’Œ `task_description`ï¼Œå¿…é¡»å…ˆé€šè¿‡ `processor` å¤„ç†æˆ inputsï¼Œç„¶åç”¨ `**inputs` è§£åŒ…ä¼ é€’ã€‚è¿™æ˜¯ä½ é‡åˆ°é”™è¯¯çš„æ ¹æœ¬åŸå› ã€‚
