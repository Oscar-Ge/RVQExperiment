"""
Robust RFSQ with LayerNorm Strategy

åŸºäºè®ºæ–‡æ”¹è¿›ï¼Œå¼•å…¥LayerNormæå‡é‡åŒ–ç²¾åº¦ã€‚

å…³é”®æ”¹è¿›ï¼š
1. æ¯å±‚é‡åŒ–å‰ï¼šLayerNormï¼ˆæ”¾å¤§å¾®å¼±ä¿¡å·ï¼‰
2. é‡åŒ–åï¼šInverse LayerNormï¼ˆè¿˜åŸå°ºåº¦ï¼‰
3. è®©L3-L7é‡æ–°æœ‰æ•ˆï¼Œæå‡ç²¾ç»†åº¦

å¼•ç”¨ï¼š
- [cite: 942] LayerNorm for signal amplification
- [cite: 944] Inverse LayerNorm for scale restoration
"""

import torch
import torch.nn as nn


class RobustSTEQuantizer(nn.Module):
    """
    æ”¹è¿›çš„STEé‡åŒ–å™¨ï¼ˆå¸¦LayerNormï¼‰

    ä¸åŸå§‹STEQuantizerçš„åŒºåˆ«ï¼š
    - âœ… æ·»åŠ LayerNormå½’ä¸€åŒ–æ®‹å·®ä¿¡å·
    - âœ… åœ¨å½’ä¸€åŒ–ç©ºé—´ä¸­é‡åŒ–
    - âœ… åå½’ä¸€åŒ–è¿˜åŸå°ºåº¦
    - âœ… ä¿æŒåå±‚çš„é‡åŒ–æœ‰æ•ˆæ€§

    Parameters:
        num_levels: é‡åŒ–çº§åˆ«æ•°ï¼ˆé»˜è®¤7ï¼‰
        use_layernorm: æ˜¯å¦ä½¿ç”¨LayerNormç­–ç•¥ï¼ˆé»˜è®¤Trueï¼‰
    """

    def __init__(self, num_levels=7, use_layernorm=True):
        super().__init__()
        self.num_levels = num_levels
        self.use_layernorm = use_layernorm

        # é‡åŒ–è¾¹ç•Œ [-1, 1]
        self.register_buffer('boundaries', torch.linspace(-1, 1, num_levels))

    def forward(self, z):
        """
        Forward pass with optional LayerNorm

        Args:
            z: [Batch, Seq, Dim] - æ®‹å·®ä¿¡å·

        Returns:
            z_q: [Batch, Seq, Dim] - é‡åŒ–åçš„å€¼ï¼ˆåŸå§‹å°ºåº¦ï¼‰
            indices: [Batch, Seq, Dim] - ç¦»æ•£ç´¢å¼• [0, num_levels-1]
        """
        if self.use_layernorm:
            # ========================================
            # è®ºæ–‡ç­–ç•¥ï¼šLayerNorm + é‡åŒ– + Inverse
            # ========================================

            # Step 1: ä¿å­˜åŸå§‹å°ºåº¦ä¿¡æ¯
            # æ¯ä¸ªdimensionç‹¬ç«‹è®¡ç®—mean/stdï¼Œä¿æŒç²¾åº¦
            original_mean = z.mean(dim=-1, keepdim=True)  # [B, S, 1]
            original_std = z.std(dim=-1, keepdim=True) + 1e-5  # [B, S, 1]

            # Step 2: å½’ä¸€åŒ– [cite: 942]
            # å°†æ®‹å·®å½’ä¸€åŒ–åˆ°ç›¸ä¼¼çš„å°ºåº¦ï¼Œæ”¾å¤§å¾®å¼±ä¿¡å·
            z_norm = (z - original_mean) / original_std  # [B, S, D]

            # Step 3: é‡åŒ–ï¼ˆåœ¨å½’ä¸€åŒ–ç©ºé—´ï¼‰
            # æ­¤æ—¶æ‰€æœ‰å±‚çš„ä¿¡å·å¼ºåº¦ç›¸ä¼¼ï¼Œé‡åŒ–æ›´æœ‰æ•ˆ
            dist = torch.abs(z_norm.unsqueeze(-1) - self.boundaries.unsqueeze(0).unsqueeze(0).unsqueeze(0))
            indices = torch.argmin(dist, dim=-1)  # [B, S, D]
            z_q_norm = self.boundaries[indices]  # [B, S, D]

            # Step 4: åå½’ä¸€åŒ– [cite: 944]
            # è¿˜åŸåˆ°åŸå§‹å°ºåº¦ï¼Œä¿æŒæ®‹å·®æ›´æ–°çš„æ­£ç¡®æ€§
            z_q = z_q_norm * original_std + original_mean  # [B, S, D]

        else:
            # ========================================
            # åŸå§‹ç­–ç•¥ï¼šç›´æ¥é‡åŒ–
            # ========================================
            dist = torch.abs(z.unsqueeze(-1) - self.boundaries.unsqueeze(0).unsqueeze(0).unsqueeze(0))
            indices = torch.argmin(dist, dim=-1)
            z_q = self.boundaries[indices]

        # Straight-Through Estimator (æ¢¯åº¦å›ä¼ )
        z_q_out = z + (z_q - z).detach()

        return z_q_out, indices


class RobustRFSQBlock(nn.Module):
    """
    æ”¹è¿›çš„RFSQ Blockï¼ˆå¤šå±‚æ®‹å·®é‡åŒ–ï¼‰

    ä¸åŸå§‹RFSQBlockçš„åŒºåˆ«ï¼š
    - âœ… ä½¿ç”¨RobustSTEQuantizeræ›¿ä»£STEQuantizer
    - âœ… æ¯å±‚éƒ½åº”ç”¨LayerNormç­–ç•¥
    - âœ… åå±‚ï¼ˆL3-L7ï¼‰é‡æ–°å˜å¾—æœ‰æ•ˆ

    Parameters:
        num_layers: å±‚æ•°ï¼ˆé»˜è®¤8ï¼‰
        num_levels: æ¯å±‚çš„é‡åŒ–çº§åˆ«ï¼ˆé»˜è®¤7ï¼‰
        use_layernorm: æ˜¯å¦ä½¿ç”¨LayerNormç­–ç•¥ï¼ˆé»˜è®¤Trueï¼‰
    """

    def __init__(self, num_layers=8, num_levels=7, use_layernorm=True):
        super().__init__()
        self.num_layers = num_layers
        self.num_levels = num_levels
        self.use_layernorm = use_layernorm

        # æ¯ä¸€å±‚éƒ½æ˜¯ç‹¬ç«‹çš„é‡åŒ–å™¨
        self.layers = nn.ModuleList([
            RobustSTEQuantizer(num_levels=num_levels, use_layernorm=use_layernorm)
            for _ in range(num_layers)
        ])

    def forward(self, z):
        """
        Residual quantization with LayerNorm

        Args:
            z: [Batch, Seq, Dim] - è¾“å…¥latent

        Returns:
            quantized_sum: [Batch, Seq, Dim] - é‡åŒ–åçš„é‡æ„
            codes: [Batch, Seq, Dim, Num_Layers] - ç¦»æ•£codes
        """
        residual = z
        quantized_sum = 0
        all_indices = []

        for layer_idx, layer in enumerate(self.layers):
            # é‡åŒ–å½“å‰æ®‹å·®
            z_q, indices = layer(residual)

            # ç´¯åŠ é‡åŒ–å€¼
            quantized_sum = quantized_sum + z_q

            # æ›´æ–°æ®‹å·®
            residual = residual - z_q

            # è®°å½•indices
            all_indices.append(indices)

            # å¯é€‰ï¼šæ‰“å°æ®‹å·®ç»Ÿè®¡ï¼ˆè°ƒè¯•ç”¨ï¼‰
            # if layer_idx % 2 == 0:
            #     print(f"  Layer {layer_idx}: residual std = {residual.std().item():.6f}")

        # Stack codes: [B, S, D, L]
        codes = torch.stack(all_indices, dim=-1)

        return quantized_sum, codes

    def decode_from_indices(self, indices):
        """
        ä»ç¦»æ•£indicesè§£ç å›è¿ç»­latent

        Args:
            indices: [Batch, Seq, Dim, Num_Layers] - ç¦»æ•£codes

        Returns:
            reconstruction: [Batch, Seq, Dim] - é‡æ„çš„latent
        """
        batch_size, seq_len, dim, num_layers = indices.shape
        assert num_layers == self.num_layers, f"Expected {self.num_layers} layers, got {num_layers}"

        # åˆå§‹åŒ–é‡æ„
        reconstruction = torch.zeros(batch_size, seq_len, dim, device=indices.device)

        # é€å±‚ç´¯åŠ 
        for layer_idx in range(num_layers):
            layer_indices = indices[:, :, :, layer_idx]  # [B, S, D]
            layer_values = self.layers[layer_idx].boundaries[layer_indices]
            reconstruction = reconstruction + layer_values

        return reconstruction


class ActionRFSQAE(nn.Module):
    """
    æ”¹è¿›çš„Action RFSQ AutoEncoder

    ä¸åŸå§‹ActionRFSQAEçš„åŒºåˆ«ï¼š
    - âœ… ä½¿ç”¨RobustRFSQBlockæ›¿ä»£RFSQBlock
    - âœ… è‡ªåŠ¨è·å¾—LayerNormçš„ç²¾åº¦æå‡
    - âœ… å…¶ä»–éƒ¨åˆ†ä¿æŒä¸å˜ï¼ˆencoder/decoderæ¶æ„ï¼‰

    Parameters:
        action_dim: åŠ¨ä½œç»´åº¦ï¼ˆé»˜è®¤7ï¼‰
        hidden_dim: latentç»´åº¦ï¼ˆé»˜è®¤16ï¼‰
        num_layers: RFSQå±‚æ•°ï¼ˆé»˜è®¤8ï¼‰
        num_levels: é‡åŒ–çº§åˆ«ï¼ˆé»˜è®¤7ï¼‰
        use_layernorm: æ˜¯å¦ä½¿ç”¨LayerNormç­–ç•¥ï¼ˆé»˜è®¤Trueï¼‰
    """

    def __init__(
        self,
        action_dim=7,
        hidden_dim=16,
        num_layers=8,
        num_levels=7,
        use_layernorm=True,
    ):
        super().__init__()
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_levels = num_levels
        self.use_layernorm = use_layernorm

        # Encoder: åŠ¨ä½œ -> latent
        self.encoder = nn.Sequential(
            nn.Linear(action_dim, 64),
            nn.Mish(),
            nn.Linear(64, hidden_dim),
            nn.Tanh()
        )

        # RFSQ Block (æ”¹è¿›ç‰ˆ)
        self.rfsq = RobustRFSQBlock(
            num_layers=num_layers,
            num_levels=num_levels,
            use_layernorm=use_layernorm,
        )

        # Decoder: latent -> åŠ¨ä½œ
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.Mish(),
            nn.Linear(64, action_dim)
        )

    def forward(self, x):
        """
        Forward pass: ç¼–ç  -> é‡åŒ– -> è§£ç 

        Args:
            x: [Batch, Seq, Action_Dim] - åŠ¨ä½œåºåˆ—

        Returns:
            x_recon: [Batch, Seq, Action_Dim] - é‡æ„çš„åŠ¨ä½œ
            codes: [Batch, Seq, Hidden_Dim, Num_Layers] - ç¦»æ•£codes
        """
        # Encode
        z = self.encoder(x)  # [B, S, Hidden]

        # Quantize (with LayerNorm if enabled)
        z_quantized, codes = self.rfsq(z)  # [B, S, Hidden], [B, S, H, L]

        # Decode
        x_recon = self.decoder(z_quantized)  # [B, S, Action]

        return x_recon, codes

    def encode(self, x):
        """ä»…ç¼–ç ï¼Œè¿”å›codes"""
        z = self.encoder(x)
        _, codes = self.rfsq(z)
        return codes

    def decode_from_indices(self, indices):
        """
        ä»ç¦»æ•£indicesè§£ç å›åŠ¨ä½œ

        Args:
            indices: [Batch, Chunk, Hidden_Dim, Num_Layers]

        Returns:
            actions: [Batch, Chunk, Action_Dim]
        """
        batch_size, chunk_len, hidden_dim, num_layers = indices.shape

        # RFSQè§£ç ï¼šindices -> latent
        z_reconstructed = self.rfsq.decode_from_indices(indices)

        # Decoderï¼šlatent -> actions
        # Reshape for decoder: [B, C, H] -> [B*C, H]
        z_flat = z_reconstructed.view(-1, self.hidden_dim)
        actions_flat = self.decoder(z_flat)

        # Reshape back: [B*C, A] -> [B, C, A]
        actions = actions_flat.view(batch_size, chunk_len, -1)

        return actions


# ============================================================
# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæ¨¡å‹
# ============================================================

def create_robust_rfsq_ae(
    action_dim=7,
    hidden_dim=16,
    num_layers=8,
    num_levels=7,
    use_layernorm=True,
    device='cuda',
):
    """
    åˆ›å»ºæ”¹è¿›çš„RFSQ AutoEncoder

    Args:
        action_dim: åŠ¨ä½œç»´åº¦
        hidden_dim: latentç»´åº¦
        num_layers: RFSQå±‚æ•°
        num_levels: é‡åŒ–çº§åˆ«
        use_layernorm: æ˜¯å¦ä½¿ç”¨LayerNormç­–ç•¥
        device: è®¾å¤‡

    Returns:
        model: ActionRFSQAEå®ä¾‹
    """
    model = ActionRFSQAE(
        action_dim=action_dim,
        hidden_dim=hidden_dim,
        num_layers=num_layers,
        num_levels=num_levels,
        use_layernorm=use_layernorm,
    )

    model = model.to(device)

    # æ‰“å°é…ç½®
    print("=" * 60)
    print("ğŸ”§ Robust RFSQ AutoEncoder Configuration")
    print("=" * 60)
    print(f"   Action Dim: {action_dim}")
    print(f"   Hidden Dim: {hidden_dim}")
    print(f"   Num Layers: {num_layers}")
    print(f"   Num Levels: {num_levels}")
    print(f"   Use LayerNorm: {use_layernorm} {'âœ… (Robust)' if use_layernorm else 'âŒ (Naive)'}")
    print(f"   Device: {device}")

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   Total Parameters: {total_params:,}")
    print("=" * 60)

    return model


# ============================================================
# å¯¹æ¯”æµ‹è¯•å‡½æ•°
# ============================================================

def compare_naive_vs_robust(action_samples, device='cuda'):
    """
    å¯¹æ¯”åŸå§‹RFSQ vs æ”¹è¿›RFSQ

    Args:
        action_samples: [Batch, Seq, 7] - æµ‹è¯•åŠ¨ä½œ
        device: è®¾å¤‡

    Returns:
        results: dict with comparison metrics
    """
    import torch.nn.functional as F

    print("\n" + "=" * 60)
    print("ğŸ“Š Comparing Naive RFSQ vs Robust RFSQ")
    print("=" * 60)

    # 1. Create models
    naive_model = create_robust_rfsq_ae(use_layernorm=False, device=device)
    robust_model = create_robust_rfsq_ae(use_layernorm=True, device=device)

    naive_model.eval()
    robust_model.eval()

    actions = torch.from_numpy(action_samples).float().to(device)

    with torch.no_grad():
        # 2. Naive RFSQ
        naive_recon, naive_codes = naive_model(actions)
        naive_mse = F.mse_loss(naive_recon, actions).item()

        # 3. Robust RFSQ
        robust_recon, robust_codes = robust_model(actions)
        robust_mse = F.mse_loss(robust_recon, actions).item()

    # 4. Results
    improvement = (naive_mse - robust_mse) / naive_mse * 100

    results = {
        'naive_mse': naive_mse,
        'robust_mse': robust_mse,
        'improvement_pct': improvement,
    }

    print(f"\n   Naive RFSQ MSE: {naive_mse:.6f}")
    print(f"   Robust RFSQ MSE: {robust_mse:.6f}")
    print(f"   Improvement: {improvement:.1f}%")
    print("=" * 60)

    return results


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("Testing Robust RFSQ implementation...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    chunk_len = 8
    action_dim = 7

    test_actions = torch.randn(batch_size, chunk_len, action_dim).to(device)

    # æµ‹è¯•Robustæ¨¡å‹
    model = create_robust_rfsq_ae(use_layernorm=True, device=device)
    model.eval()

    with torch.no_grad():
        recon, codes = model(test_actions)

        print(f"\nâœ… Input shape: {test_actions.shape}")
        print(f"âœ… Reconstruction shape: {recon.shape}")
        print(f"âœ… Codes shape: {codes.shape}")

        mse = F.mse_loss(recon, test_actions).item()
        print(f"âœ… Random init MSE: {mse:.6f}")

    print("\nâœ… All tests passed!")
