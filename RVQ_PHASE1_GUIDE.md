# Phase 1: RVQ Tokenizer - Complete Guide

**ç›®æ ‡**: è®­ç»ƒä¸€ä¸ª RVQ (Residual Vector Quantization) tokenizerï¼ŒéªŒè¯æœºå™¨äººåŠ¨ä½œçš„å±‚çº§ç»“æ„å‡è®¾ã€‚

**æ—¶é—´**: é¢„è®¡ 1-2 å‘¨ï¼ˆå®é™…è¿è¡Œæ—¶é—´çº¦ 2-3 å°æ—¶ï¼‰

---

## ğŸ¯ Phase 1 ç›®æ ‡

1. **è®­ç»ƒ VQ-VAE**: å°† (T, 7) åŠ¨ä½œç¼–ç ä¸º 8 å±‚ RVQ codes
2. **éªŒè¯å±‚çº§å‡è®¾**:
   - Layer 1-2 (ç²—ç•¥) â†’ MSE â‰ˆ 0.01
   - Layer 1-8 (ç²¾ç»†) â†’ MSE < 0.001
3. **å¯¹æ¯” DCT**: è¯æ˜ RVQ ä¼˜äº DCT

---

## ğŸ“‹ å‰ç½®è¦æ±‚

### 1. å·²å®Œæˆ DCT å®éªŒ
```bash
# ç¡®ä¿ DCT å®éªŒå·²ç»è·‘é€š
python test_dct_compression.py
python analyze_libero_actions.py --num_episodes 20
```

### 2. ç¯å¢ƒè¦æ±‚
- **DCT å®éªŒçš„æ‰€æœ‰ä¾èµ–** +
- **PyTorch** (å·²å®‰è£…ï¼Œç”¨äº Ï€0.5)
- **CUDA GPU** (æ¨èï¼ŒCPU è®­ç»ƒä¼šå¾ˆæ…¢)

### 3. éªŒè¯ç¯å¢ƒ
```bash
# æ£€æŸ¥ PyTorch
python -c "import torch; print(f'PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}')"

# åº”è¯¥è¾“å‡ºç±»ä¼¼:
# PyTorch 2.x.x, CUDA: True
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3 æ­¥èµ°ï¼‰

### Step 1: å•å…ƒæµ‹è¯•ï¼ˆ5 åˆ†é’Ÿï¼‰

**ç›®çš„**: éªŒè¯ RVQ tokenizer å®ç°æ­£ç¡®

```bash
python test_rvq_tokenizer.py
```

**é¢„æœŸè¾“å‡º**:
```
================================================================================
RVQ TOKENIZER TEST SUITE
================================================================================
================================================================================
TEST 1: VECTOR QUANTIZER
================================================================================

Input shape: torch.Size([2, 10, 64])
Quantized shape: torch.Size([2, 10, 64])
Indices shape: torch.Size([2, 10])
VQ loss: 0.234567

âœ… VectorQuantizer test passed!

================================================================================
TEST 2: RESIDUAL VECTOR QUANTIZER
================================================================================
...

================================================================================
TEST SUMMARY
================================================================================
âœ… All tests passed!

Next steps:
  1. Run: python train_rvq_tokenizer.py --num_episodes 50 --epochs 100
  2. Train RVQ tokenizer on real LIBERO actions
  3. Run: python analyze_rvq_compression.py --model rvq_tokenizer.pt
  4. Compare results with DCT compression
================================================================================
```

---

### Step 2: è®­ç»ƒ RVQ Tokenizerï¼ˆ30-60 åˆ†é’Ÿï¼‰

**ç›®çš„**: åœ¨çœŸå® LIBERO åŠ¨ä½œä¸Šè®­ç»ƒ RVQ

```bash
# åŸºç¡€è®­ç»ƒï¼ˆ50 episodesï¼Œ100 epochsï¼‰
python train_rvq_tokenizer.py \
    --task_suite libero_spatial \
    --num_episodes 50 \
    --epochs 100 \
    --batch_size 32 \
    --device cuda

# å¦‚æœæƒ³å¿«é€ŸéªŒè¯ï¼ˆå‡å°‘æ—¶é—´ï¼‰
python train_rvq_tokenizer.py \
    --num_episodes 20 \
    --epochs 50 \
    --device cuda
```

**é¢„æœŸè¾“å‡º**:
```
================================================================================
RVQ TOKENIZER TRAINING
================================================================================
================================================================================
COLLECTING Ï€0.5 ACTIONS ON LIBERO
================================================================================

[1/4] Loading Ï€0.5 policy...
âœ“ Policy loaded successfully!

[2/4] Loading LIBERO task: libero_spatial - Task 0
  Task: pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate
  Description: pick up the black bowl between the plate and the ramekin and place it on the plate

[3/4] Creating LIBERO environment...

[4/4] Collecting actions from 50 episodes...
  Episode 1/50: âœ… (steps: 88)
  Episode 2/50: âœ… (steps: 138)
  ...

âœ… Collected 2187 action chunks
   Success rate: 96.0%

================================================================================
TRAINING RVQ TOKENIZER
================================================================================

Dataset:
  Action chunks: 2187
  Chunk size: 10
  Action dim: 7

Model config:
  Num layers: 8
  Hidden dim: 64
  Codebook size: 256
  Residual dropout: 0.1

Training config:
  Epochs: 100
  Batch size: 32
  Learning rate: 0.001
  Device: cuda

================================================================================
TRAINING PROGRESS
================================================================================
Epoch 10/100:
  Recon Loss: 0.012345
  VQ Loss: 0.234567
  Total Loss: 0.246912

Epoch 20/100:
  Recon Loss: 0.005678
  VQ Loss: 0.198765
  Total Loss: 0.204443

...

Epoch 100/100:
  Recon Loss: 0.000234
  VQ Loss: 0.156789
  Total Loss: 0.157023

âœ… Model saved to rvq_tokenizer.pt

ğŸ“Š Training curves saved to training_history.png

================================================================================
QUICK VALIDATION
================================================================================

Test reconstruction MSE: 0.000456
âœ… Validation passed! MSE < 0.01

================================================================================
NEXT STEPS
================================================================================
âœ… RVQ tokenizer trained successfully!

Model saved to: rvq_tokenizer.pt

Next:
  1. Run: python analyze_rvq_compression.py --model rvq_tokenizer.pt
  2. Test different numbers of layers (1-8)
  3. Plot MSE vs. number of layers
================================================================================
```

**è®­ç»ƒæ—¶é—´ä¼°è®¡**:
- 50 episodes collection: 15-20 åˆ†é’Ÿ
- 100 epochs training: 10-30 åˆ†é’Ÿï¼ˆå–å†³äº GPUï¼‰
- æ€»è®¡: 30-60 åˆ†é’Ÿ

---

### Step 3: åˆ†æ RVQ å‹ç¼©ï¼ˆ10 åˆ†é’Ÿï¼‰

**ç›®çš„**: æµ‹è¯•ä¸åŒå±‚æ•°çš„é‡å»ºè´¨é‡ï¼Œå¤ç°ç±»ä¼¼ DCT çš„å›¾è¡¨

```bash
# åˆ†æè®­ç»ƒå¥½çš„æ¨¡å‹
python analyze_rvq_compression.py \
    --model rvq_tokenizer.pt \
    --task_suite libero_spatial \
    --num_episodes 20 \
    --device cuda
```

**é¢„æœŸè¾“å‡º**:
```
================================================================================
RVQ COMPRESSION ANALYSIS
================================================================================

[1/3] Loading trained RVQ model...
âœ“ Loaded RVQ tokenizer from rvq_tokenizer.pt
  Config: {'action_dim': 7, 'chunk_size': 10, 'num_layers': 8, ...}

[2/3] Collecting test actions...
âœ… Collected 874 action chunks

[3/3] Analyzing compression...
================================================================================
ANALYZING RVQ COMPRESSION
================================================================================

Layers=1:
  MSE: 0.023456 Â± 0.008923
  Tokens: 10.0
  Compression: 28.00x

Layers=2:
  MSE: 0.008765 Â± 0.003456
  Tokens: 20.0
  Compression: 14.00x

Layers=3:
  MSE: 0.004321 Â± 0.001789
  Tokens: 30.0
  Compression: 9.33x

Layers=4:
  MSE: 0.002109 Â± 0.000987
  Tokens: 40.0
  Compression: 7.00x

Layers=5:
  MSE: 0.001234 Â± 0.000567
  Tokens: 50.0
  Compression: 5.60x

Layers=6:
  MSE: 0.000789 Â± 0.000345
  Tokens: 60.0
  Compression: 4.67x

Layers=7:
  MSE: 0.000456 Â± 0.000234
  Tokens: 70.0
  Compression: 4.00x

Layers=8:
  MSE: 0.000123 Â± 0.000089
  Tokens: 80.0
  Compression: 3.50x

ğŸ“Š Plot saved to rvq_compression_analysis.png

================================================================================
SUMMARY
================================================================================

âœ… Optimal setting: 3 RVQ layers
   MSE: 0.004321 Â± 0.001789
   Compression: 9.33x
   Tokens per chunk: 30.0

ğŸ¯ Best compression with excellent MSE (<0.01):
   3 layers
   MSE: 0.004321
   Compression: 9.33x
   â†’ Coarse layers (1-3) capture key motion!

ğŸ“Š Layer comparison:
   Layer 1-2 (coarse): MSE=0.008765
   Layer 1-8 (fine):   MSE=0.000123
   â†’ Improvement from fine layers: 98.6%

âœ… HYPOTHESIS VALIDATED!
   Layer 1-2 alone achieve MSE < 0.01
   â†’ Can use coarse layers for simple motions
   â†’ Only activate fine layers for complex tasks

================================================================================
NEXT STEPS
================================================================================
âœ… RVQ compression analysis complete!

Based on results:
  âœ… Ready for Phase 2: Train VLA policy to predict RVQ tokens
  âœ… Ready for Phase 3: Implement adaptive inference
     - Use layers 1-3 for coarse prediction
     - Activate all layers when uncertainty is high
================================================================================
```

**ç”Ÿæˆçš„å›¾è¡¨**: `rvq_compression_analysis.png`
- å·¦å›¾: MSE vs. RVQ Layers (å¯¹æ•°å°ºåº¦)
- å³å›¾: Compression Ratio vs. RVQ Layers

---

## ğŸ“Š é¢„æœŸç»“æœ

### âœ… æˆåŠŸçš„æ ‡å‡†

**Hypothesis 1: å±‚çº§å¯¹åº”å¤æ‚åº¦**
```
Layer 1-2: MSE â‰ˆ 0.01 (ç²—ç•¥è¿åŠ¨)
Layer 3-4: MSE â‰ˆ 0.001 (ä¸­ç­‰ç²¾åº¦)
Layer 5-8: MSE < 0.0001 (ç²¾ç»†ä¿®æ­£)
```

**Hypothesis 2: ä¼˜äº DCT**
```
RVQ (3 layers): MSE â‰ˆ 0.004, Compression = 9.33x
DCT (keep=3):   MSE â‰ˆ 0.007, Compression = 3.33x

â†’ RVQ åŒç­‰ MSE ä¸‹ï¼Œå‹ç¼©æ¯”æ›´é«˜ âœ…
```

---

## ğŸ“ˆ ä¸ DCT å¯¹æ¯”

| æ–¹æ³• | MSE < 0.01 æ—¶å±‚æ•° | å‹ç¼©æ¯” | ä¼˜åŠ¿ |
|------|------------------|--------|------|
| **DCT** | 3 ç³»æ•° | 3.33x | ç®€å•ï¼Œæ— éœ€è®­ç»ƒ |
| **RVQ** | 2-3 å±‚ | 9-14x | æ›´é«˜å‹ç¼©ï¼Œå­¦ä¹ æ•°æ®åˆ†å¸ƒ |

---

## ğŸ”¬ é«˜çº§ç”¨æ³•

### 1. è°ƒæ•´è¶…å‚æ•°

```bash
# å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆå¦‚æœ MSE ä¸å¤Ÿä½ï¼‰
python train_rvq_tokenizer.py \
    --num_episodes 100 \
    --hidden_dim 128 \
    --codebook_size 512 \
    --epochs 200

# å‡å°‘æ¨¡å‹å¤§å°ï¼ˆå¦‚æœè¿‡æ‹Ÿåˆï¼‰
python train_rvq_tokenizer.py \
    --hidden_dim 32 \
    --codebook_size 128 \
    --epochs 50
```

### 2. æµ‹è¯•ä¸åŒä»»åŠ¡

```bash
# Spatial tasks
python train_rvq_tokenizer.py --task_suite libero_spatial --task_id 0

# Object tasks (æ›´å¤æ‚)
python train_rvq_tokenizer.py --task_suite libero_object --task_id 2

# Long-horizon tasks
python train_rvq_tokenizer.py --task_suite libero_10 --task_id 0
```

### 3. ä½¿ç”¨ CPUï¼ˆå¦‚æœæ²¡æœ‰ GPUï¼‰

```bash
python train_rvq_tokenizer.py \
    --num_episodes 20 \
    --epochs 50 \
    --batch_size 16 \
    --device cpu
```

**æ³¨æ„**: CPU è®­ç»ƒä¼šæ…¢ 10-20 å€ã€‚

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: è®­ç»ƒ Loss ä¸ä¸‹é™

**ç—‡çŠ¶**:
```
Epoch 50/100:
  Recon Loss: 0.123456 (æ²¡å˜åŒ–)
  VQ Loss: 0.234567
```

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ  learning rate:
   ```bash
   python train_rvq_tokenizer.py --lr 5e-3
   ```
2. å‡å°‘ residual dropout:
   ```bash
   python train_rvq_tokenizer.py --residual_dropout 0.0
   ```
3. å¢åŠ æ¨¡å‹å®¹é‡:
   ```bash
   python train_rvq_tokenizer.py --hidden_dim 128
   ```

---

### é—®é¢˜ 2: MSE å§‹ç»ˆ > 0.01

**ç—‡çŠ¶**:
```
Layers=2:
  MSE: 0.035678 (å¤ªé«˜)
```

**è§£å†³æ–¹æ¡ˆ**:
1. **è®­ç»ƒæ›´é•¿æ—¶é—´**:
   ```bash
   python train_rvq_tokenizer.py --epochs 200
   ```
2. **å¢åŠ æ•°æ®é‡**:
   ```bash
   python train_rvq_tokenizer.py --num_episodes 100
   ```
3. **å¢åŠ  codebook size**:
   ```bash
   python train_rvq_tokenizer.py --codebook_size 512
   ```
4. **æ£€æŸ¥æ•°æ®åˆ†å¸ƒ**: å¯èƒ½æŸäº›ä»»åŠ¡ç‰¹åˆ«å¤æ‚ï¼Œéœ€è¦æ›´å¤šå±‚

---

### é—®é¢˜ 3: GPU å†…å­˜ä¸è¶³

**ç—‡çŠ¶**:
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°‘ batch size
python train_rvq_tokenizer.py --batch_size 16

# å‡å°‘æ¨¡å‹å¤§å°
python train_rvq_tokenizer.py --hidden_dim 32 --batch_size 16

# ä½¿ç”¨ CPU
python train_rvq_tokenizer.py --device cpu --num_episodes 20
```

---

### é—®é¢˜ 4: æµ‹è¯•æ—¶æ¨¡å‹åŠ è½½å¤±è´¥

**ç—‡çŠ¶**:
```
FileNotFoundError: rvq_tokenizer.pt not found
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦è®­ç»ƒå®Œæˆ
ls -lh rvq_tokenizer.pt

# å¦‚æœä¸å­˜åœ¨ï¼Œé‡æ–°è®­ç»ƒ
python train_rvq_tokenizer.py --num_episodes 50 --epochs 100

# æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„
python analyze_rvq_compression.py --model /path/to/rvq_tokenizer.pt
```

---

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

### 1. `rvq_tokenizer.py`
**åŠŸèƒ½**: RVQ tokenizer å®ç°

**å…³é”®ç±»**:
- `VectorQuantizer`: å•å±‚ VQï¼Œä½¿ç”¨ EMA æ›´æ–° codebook
- `ResidualVectorQuantizer`: å¤šå±‚ RVQï¼Œæ®‹å·®ç´¯ç§¯
- `RVQTokenizer`: å®Œæ•´æ¨¡å‹ï¼ˆencoder + RVQ + decoderï¼‰

**é…ç½®å‚æ•°**:
```python
RVQTokenizer(
    action_dim=7,           # åŠ¨ä½œç»´åº¦
    chunk_size=10,          # åŠ¨ä½œå—å¤§å°
    num_layers=8,           # RVQ å±‚æ•° (æ ¸å¿ƒå‚æ•°)
    hidden_dim=64,          # éšè—å±‚ç»´åº¦
    num_embeddings=256,     # Codebook å¤§å°
    commitment_cost=0.25,   # Commitment loss æƒé‡
)
```

---

### 2. `train_rvq_tokenizer.py`
**åŠŸèƒ½**: è®­ç»ƒè„šæœ¬

**ä¸»è¦æ­¥éª¤**:
1. æ”¶é›† Ï€0.5 åŠ¨ä½œï¼ˆå¤ç”¨ `collect_pi05_actions()`ï¼‰
2. åˆ›å»º RVQTokenizer
3. è®­ç»ƒå¾ªç¯ï¼ˆreconstruction loss + VQ lossï¼‰
4. ä¿å­˜æ¨¡å‹

**å‘½ä»¤è¡Œå‚æ•°**:
```bash
--task_suite      # LIBERO ä»»åŠ¡å¥—ä»¶
--num_episodes    # æ”¶é›†å¤šå°‘ episodes
--num_layers      # RVQ å±‚æ•° (default: 8)
--hidden_dim      # éšè—ç»´åº¦ (default: 64)
--codebook_size   # Codebook å¤§å° (default: 256)
--epochs          # è®­ç»ƒ epochs (default: 100)
--batch_size      # Batch size (default: 32)
--lr              # Learning rate (default: 1e-3)
--residual_dropout # Residual dropout (default: 0.1)
--device          # cuda or cpu
--output          # è¾“å‡ºæ¨¡å‹è·¯å¾„
```

---

### 3. `analyze_rvq_compression.py`
**åŠŸèƒ½**: åˆ†æè„šæœ¬ï¼Œç±»ä¼¼ `analyze_libero_actions.py`

**ä¸»è¦æ­¥éª¤**:
1. åŠ è½½è®­ç»ƒå¥½çš„ RVQ tokenizer
2. æ”¶é›†æµ‹è¯•åŠ¨ä½œ
3. æµ‹è¯•ä¸åŒå±‚æ•°ï¼ˆ1-8ï¼‰çš„é‡å»ºè¯¯å·®
4. ç”Ÿæˆå›¾è¡¨ï¼ˆMSE vs. Layersï¼‰

**è¾“å‡º**:
- `rvq_compression_analysis.png`: å‹ç¼©åˆ†æå›¾
- ç»ˆç«¯è¾“å‡ºï¼šæ¯å±‚çš„ MSEã€å‹ç¼©æ¯”ã€å‡è®¾éªŒè¯ç»“æœ

---

### 4. `test_rvq_tokenizer.py`
**åŠŸèƒ½**: å•å…ƒæµ‹è¯•

**åŒ…å«æµ‹è¯•**:
- `test_vector_quantizer()`: æµ‹è¯•å•å±‚ VQ
- `test_residual_vector_quantizer()`: æµ‹è¯•å¤šå±‚ RVQ
- `test_rvq_tokenizer_basic()`: æµ‹è¯•ç¼–ç è§£ç 
- `test_rvq_tokenizer_layers()`: æµ‹è¯•ä¸åŒå±‚æ•°
- `test_realistic_smooth_actions()`: æµ‹è¯•å¹³æ»‘è½¨è¿¹

---

## ğŸ“ ç†è§£ RVQ

### RVQ vs. DCT

**DCT (Discrete Cosine Transform)**:
- å›ºå®šå˜æ¢åŸºï¼ˆä½™å¼¦å‡½æ•°ï¼‰
- ä¸éœ€è¦è®­ç»ƒ
- é€‚ç”¨äºæ‰€æœ‰å¹³æ»‘ä¿¡å·

**RVQ (Residual Vector Quantization)**:
- å­¦ä¹ çš„ codebookï¼ˆä»æ•°æ®ä¸­å­¦ä¹ ï¼‰
- éœ€è¦è®­ç»ƒ
- èƒ½æ•æ‰æ•°æ®ç‰¹æœ‰çš„æ¨¡å¼

### RVQ å¦‚ä½•å·¥ä½œ

```python
# Layer 1: ç¼–ç ä¸»è¦ä¿¡æ¯
quantized_1 = VQ_1(input)
residual_1 = input - quantized_1

# Layer 2: ç¼–ç æ®‹å·®
quantized_2 = VQ_2(residual_1)
residual_2 = residual_1 - quantized_2

# Layer 3: ç»§ç»­ç¼–ç æ®‹å·®
quantized_3 = VQ_3(residual_2)
...

# æœ€ç»ˆé‡å»º
reconstructed = quantized_1 + quantized_2 + quantized_3 + ...
```

### ä¸ºä»€ä¹ˆæ˜¯å±‚çº§çš„ï¼Ÿ

- **Layer 1-2**: Codebook å­¦ä¹ ä¸»è¦è¿åŠ¨æ¨¡å¼ï¼ˆå‘å‰ã€å‘åã€æŠ“å–ï¼‰
- **Layer 3-4**: Codebook å­¦ä¹ ä¿®æ­£æ¨¡å¼ï¼ˆå¾®è°ƒä½ç½®ï¼‰
- **Layer 5-8**: Codebook å­¦ä¹ ç»†èŠ‚ï¼ˆæŠ–åŠ¨ã€æ¥è§¦åŠ›ï¼‰

---

## âœ… Phase 1 å®Œæˆæ ‡å‡†

### å¿…é¡»è¾¾åˆ°çš„æŒ‡æ ‡

1. âœ… **è®­ç»ƒæˆåŠŸ**:
   - Reconstruction loss æ”¶æ•›
   - VQ loss ç¨³å®š

2. âœ… **é‡å»ºè´¨é‡**:
   - Layer 1-2: MSE < 0.01 (å¯æ¥å—)
   - Layer 1-8: MSE < 0.001 (ä¼˜ç§€)

3. âœ… **å‡è®¾éªŒè¯**:
   - å‰å‡ å±‚æ•æ‰ç²—ç•¥è¿åŠ¨
   - æ·±å±‚æ•æ‰ç²¾ç»†ä¿®æ­£

### å¯é€‰çš„é¢å¤–éªŒè¯

1. **å¯è§†åŒ–**: ç»˜åˆ¶åŸå§‹åŠ¨ä½œ vs. é‡å»ºåŠ¨ä½œ
2. **Codebook åˆ†æ**: ç»Ÿè®¡æ¯ä¸ª code çš„ä½¿ç”¨é¢‘ç‡
3. **ä»»åŠ¡é˜¶æ®µåˆ†æ**: ä¸åŒé˜¶æ®µï¼ˆreach, grasp, placeï¼‰éœ€è¦å¤šå°‘å±‚

---

## ğŸ“š ç›¸å…³æ¦‚å¿µ

### VQ-VAE (Vector Quantized Variational Autoencoder)
- **è®ºæ–‡**: [Neural Discrete Representation Learning (van den Oord et al., 2017)](https://arxiv.org/abs/1711.00937)
- **æ ¸å¿ƒæ€æƒ³**: ç”¨ç¦»æ•£ codebook æ›¿ä»£è¿ç»­æ½œåœ¨ç©ºé—´

### RVQ (Residual Vector Quantization)
- **è®ºæ–‡**: [SoundStream (Zeghidour et al., 2021)](https://arxiv.org/abs/2107.03312)
- **æ ¸å¿ƒæ€æƒ³**: å¤šå±‚ VQï¼Œæ¯å±‚é‡åŒ–å‰ä¸€å±‚çš„æ®‹å·®

### æœºå™¨äººå­¦ä¸­çš„ VQ
- **RDT-2**: ä½¿ç”¨ RVQ ç¼–ç åŠ¨ä½œ
- **VQ-VLA**: è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ + VQ

---

## ğŸ¯ Phase 2 é¢„è§ˆ

å¦‚æœ Phase 1 æˆåŠŸï¼ˆMSE è¾¾æ ‡ï¼‰ï¼Œæ¥ä¸‹æ¥ï¼š

1. **è®­ç»ƒ VLA Policy**:
   - è¾“å…¥ï¼šImage + Language + State
   - è¾“å‡ºï¼šRVQ tokens (1-8 å±‚)

2. **å®ç°è‡ªé€‚åº”æ¨ç†**:
   - Monitor: æ£€æµ‹ä»»åŠ¡å¤æ‚åº¦ï¼ˆç†µå€¼ã€å…‰æµã€è·ç¦»ï¼‰
   - Draft: ç”¨ Layer 1-2 å¿«é€Ÿé¢„æµ‹
   - Refine: å¤æ‚æ—¶æ¿€æ´» Layer 3-8

3. **Benchmark**:
   - å¯¹æ¯” Dense RVQ baseline
   - æµ‹é‡åŠ é€Ÿæ¯”å’ŒæˆåŠŸç‡

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. **æ£€æŸ¥å•å…ƒæµ‹è¯•**:
   ```bash
   python test_rvq_tokenizer.py
   ```

2. **æŸ¥çœ‹è®­ç»ƒæ›²çº¿**:
   - æ‰“å¼€ `training_history.png`
   - æ£€æŸ¥ loss æ˜¯å¦æ”¶æ•›

3. **å‡å°é—®é¢˜è§„æ¨¡**:
   ```bash
   # ç”¨æ›´å°‘çš„æ•°æ®å¿«é€ŸéªŒè¯
   python train_rvq_tokenizer.py --num_episodes 10 --epochs 20
   ```

4. **å¯¹æ¯” DCT ç»“æœ**:
   - DCT å·²ç»è¯æ˜åŠ¨ä½œæ˜¯å¯å‹ç¼©çš„
   - RVQ åº”è¯¥èƒ½è¾¾åˆ°ç±»ä¼¼æˆ–æ›´å¥½çš„ç»“æœ

---

## ğŸ“„ å¼•ç”¨

å¦‚æœä½¿ç”¨è¿™ä¸ªä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{rvq_tokenizer_2025,
  title={RVQ Tokenizer for Robot Action Compression},
  author={Your Name},
  year={2025},
  note={Phase 1 of Residual Speculative Decoding for VLA}
}
```

---

**é¢„æœŸæ—¶é—´çº¿**:
- å•å…ƒæµ‹è¯•: 5 åˆ†é’Ÿ
- è®­ç»ƒ: 30-60 åˆ†é’Ÿ
- åˆ†æ: 10 åˆ†é’Ÿ
- **æ€»è®¡: 1-2 å°æ—¶**

**æˆåŠŸæ ‡å¿—**: ç”Ÿæˆçš„ `rvq_compression_analysis.png` æ˜¾ç¤º Layer 1-2 çš„ MSE < 0.01

**ä¸‹ä¸€æ­¥**: Phase 2 - è®­ç»ƒ VLA Policy é¢„æµ‹ RVQ tokens

---

ğŸš€ **å¼€å§‹ Phase 1 å§ï¼ç¥å®éªŒé¡ºåˆ©ï¼**
